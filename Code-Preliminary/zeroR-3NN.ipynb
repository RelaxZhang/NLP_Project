{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial.distance import cdist\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nlp_function import pick_random_keys, stopwords_func, lower_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in data\n",
    "# Read in training data (claim)\n",
    "with open('../project-data/train-claims.json', 'r') as tclaim_file:\n",
    "    tclaim_data = json.load(tclaim_file)\n",
    "\n",
    "# Read in development data (claim)\n",
    "with open('../project-data/dev-claims.json', 'r') as dclaim_file:\n",
    "    dclaim_data = json.load(dclaim_file)\n",
    "\n",
    "# Read in test data (claim)\n",
    "with open('../project-data/test-claims-unlabelled.json', 'r') as uclaim_file:\n",
    "    uclaim_data = json.load(uclaim_file)\n",
    "\n",
    "# Read in evidence data\n",
    "with open('../project-data/evidence.json', 'r') as evi_file:\n",
    "    evi_data = json.load(evi_file)\n",
    "\n",
    "## Preprocessing - Lowercase operation of the case\n",
    "tclaim_data = lower_processing(tclaim_data, \"claim_text\")\n",
    "dclaim_data = lower_processing(dclaim_data, \"claim_text\")\n",
    "uclaim_data = lower_processing(uclaim_data, \"claim_text\")\n",
    "evi_data = lower_processing(evi_data, 'evidence')\n",
    "\n",
    "# ## Remove stopwords from claims and evidence (optional)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tclaim_data = stopwords_func(stop_words, \"claim\", tclaim_data)\n",
    "dclaim_data = stopwords_func(stop_words, \"claim\", dclaim_data)\n",
    "uclaim_data = stopwords_func(stop_words, \"claim\", uclaim_data)\n",
    "evi_data = stopwords_func(stop_words, \"evidence\", evi_data)\n",
    "\n",
    "# ## Create claim-evidence pair based on training set\n",
    "# train_pairs = []\n",
    "# for i in tclaim_data.values():\n",
    "#     for j in i[\"evidences\"]:\n",
    "#         train_pairs.append((i[\"claim_text\"], evi_data[j], 1))\n",
    "\n",
    "# ## insert negative sample to the training set\n",
    "# for i in tclaim_data.values():\n",
    "#     excluded_keys = i[\"evidences\"]\n",
    "#     random_keys = pick_random_keys(evi_data, excluded_keys, len(excluded_keys))\n",
    "#     for j in random_keys:\n",
    "#         train_pairs.append((i[\"claim_text\"], evi_data[j], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Obtain sentence list\n",
    "sentence_dict = {\"train\": 0, \"test\": 0}\n",
    "sentence_list = []\n",
    "for i in tclaim_data:\n",
    "    sentence_dict[\"train\"] += 1\n",
    "    sentence_list.append(tclaim_data[i][\"claim_text\"])\n",
    "for i in uclaim_data:\n",
    "    sentence_dict[\"test\"] += 1\n",
    "    sentence_list.append(uclaim_data[i][\"claim_text\"])\n",
    "\n",
    "# Start embedding with bert\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# List of sentences\n",
    "sentences = sentence_list\n",
    "\n",
    "# Tokenize and encode the sentences\n",
    "encoded_inputs = tokenizer.batch_encode_plus(\n",
    "    sentences,\n",
    "    add_special_tokens=True,\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Generate sentence embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_inputs)\n",
    "    sentence_embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Embed sentences and obtain test set vectors\n",
    "embeddings = sentence_embeddings.numpy()\n",
    "train_matrix = embeddings[:sentence_dict[\"train\"]]\n",
    "test_matrix = embeddings[sentence_dict[\"train\"]:]\n",
    "\n",
    "# Capture the closest training instance (index) to the test set\n",
    "test_train_index = []\n",
    "best_train_index = []\n",
    "for i in range(test_matrix.shape[0]):\n",
    "    distances = cdist(train_matrix, np.expand_dims(test_matrix[i], axis=0), metric='euclidean')\n",
    "    best_train_index.append(np.argmin(distances))\n",
    "    distances_flat = distances.flatten()\n",
    "    indices = np.argpartition(distances_flat, 3)[:3]\n",
    "    test_train_index.append(indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-R Classification\n",
    "label_list = []\n",
    "for i in tclaim_data.values():\n",
    "    label_list.append(i[\"claim_label\"])\n",
    "strings = label_list\n",
    "counter = Counter(strings)\n",
    "most_common = counter.most_common(1)\n",
    "most_frequent_string = most_common[0][0]\n",
    "frequency = most_common[0][1]\n",
    "\n",
    "# Assign label and evidence to the test set\n",
    "train_key_list = list(tclaim_data.keys())\n",
    "count = 0\n",
    "for i in uclaim_data.values():\n",
    "    i[\"claim_label\"] = tclaim_data[train_key_list[best_train_index[count]]][\"claim_label\"]\n",
    "    evidence_list = []\n",
    "    for j in test_train_index[count]:\n",
    "        evidence_list += tclaim_data[train_key_list[j]][\"evidences\"]\n",
    "    i[\"evidences\"] = evidence_list\n",
    "    count += 1\n",
    "\n",
    "# Save the test set result\n",
    "file_path = '../test-claims-predictions.json'\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(uclaim_data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding: (768,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Step 1: Load pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Step 2: Prepare input sentence\n",
    "sentence = \"This is an example sentence.\"\n",
    "\n",
    "# Step 3: Tokenize and encode the sentence\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Step 4: Generate word embeddings\n",
    "input_ids = torch.tensor([input_ids])\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    word_embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "# Step 5: Combine word embeddings to get sentence embedding\n",
    "sentence_embedding = torch.mean(word_embeddings, dim=0)\n",
    "\n",
    "# Step 6: Optional - Convert to NumPy array or other formats\n",
    "sentence_embedding = sentence_embedding.numpy()\n",
    "\n",
    "# Step 7: Print the sentence embedding\n",
    "print(\"Sentence embedding:\", (sentence_embedding.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain sentence list\n",
    "sentence_dict = {\"train\": 0, \"test\": 0}\n",
    "sentence_list = []\n",
    "for i in tclaim_data:\n",
    "    sentence_dict[\"train\"] += 1\n",
    "    sentence_list.append(tclaim_data[i][\"claim_text\"])\n",
    "for i in uclaim_data:\n",
    "    sentence_dict[\"test\"] += 1\n",
    "    sentence_list.append(uclaim_data[i][\"claim_text\"])\n",
    "\n",
    "# Start embedding with bert\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# List of sentences\n",
    "sentences = sentence_list\n",
    "\n",
    "# Tokenize and encode the sentences\n",
    "encoded_inputs = tokenizer.batch_encode_plus(\n",
    "    sentences,\n",
    "    add_special_tokens=True,\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Generate sentence embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_inputs)\n",
    "    sentence_embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Embed sentences and obtain test set vectors\n",
    "embeddings = sentence_embeddings.numpy()\n",
    "train_matrix = embeddings[:sentence_dict[\"train\"]]\n",
    "test_matrix = embeddings[sentence_dict[\"train\"]:]\n",
    "\n",
    "# Capture the closest training instance (index) to the test set\n",
    "test_train_index = []\n",
    "best_train_index = []\n",
    "for i in range(test_matrix.shape[0]):\n",
    "    distances = cdist(train_matrix, np.expand_dims(test_matrix[i], axis=0), metric='euclidean')\n",
    "    best_train_index.append(np.argmin(distances))\n",
    "    distances_flat = distances.flatten()\n",
    "    indices = np.argpartition(distances_flat, 3)[:3]\n",
    "    test_train_index.append(indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83558501800f4548a88d73cca1470755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b595f0eb58e4d6c838cb16809151976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ca7126368d4b04b84fcda5b0408c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d421aa2792604c37bc19d2a4a3b9ac20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "# 加载预训练的RoBERTa模型和分词器\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "# 示例句子\n",
    "sentence = \"This is an example sentence.\"\n",
    "\n",
    "# 分词和编码句子\n",
    "encoded_input = tokenizer.encode_plus(\n",
    "    sentence,\n",
    "    add_special_tokens=True,\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# 生成句子嵌入\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "    sentence_embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "# 将结果转换为numpy数组（如果需要的话）\n",
    "sentence_embeddings = sentence_embeddings.numpy()\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
