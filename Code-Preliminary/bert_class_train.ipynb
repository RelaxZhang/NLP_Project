{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ba07e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "__shop__ = \"AINLP\"\n",
    "__link__ = \"https://shop128061183.taobao.com/\"\n",
    "__date__ = 2023 / 5 / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf2e19",
   "metadata": {},
   "source": [
    "### 1. 安装所需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac0ebf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install torch -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install pandas -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install scikit_learn -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9fdacff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AdamW, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    filename=\"bert.log\",\n",
    "    filemode=\"w\",\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    ")\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f329d5-ae42-4507-94e8-5ded838cb4b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 9999\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dcaacf",
   "metadata": {},
   "source": [
    "### 2.分词器，wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a4cf0dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# huggingface\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675bf3ec",
   "metadata": {},
   "source": [
    "### 3.读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6224b492-c117-447d-8d9a-09a67afbfc9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_evidence_num(json_file):\n",
    "    f_json = json.load(open(json_file))\n",
    "    evidences = []\n",
    "    for k, v in f_json.items():\n",
    "        evidences.extend(v[\"evidences\"])\n",
    "    return set(evidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeada601-fbca-41af-81c8-daf3cd5c697e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_evidences = get_evidence_num(\"project-data/train-claims.json\")\n",
    "dev_evidences = get_evidence_num(\"project-data/dev-claims.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b1146be-a79b-4be6-b340-190df3c09c83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_evidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a31ec70-2d4a-4d91-a097-029755e6e3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3443"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_evidences = list(train_evidences | dev_evidences)\n",
    "len(all_evidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aee55815-f1d1-402d-bffe-9c5a6a190d86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_evidence_data(json_file):\n",
    "    f_json = json.load(open(json_file))\n",
    "    return f_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6578cfec-630d-49a2-888c-96717d861bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evidence_dict = get_evidence_data(\"project-data/evidence.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "503154a2-f9df-4819-a3fd-7b5b14635a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1208827"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evidence_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2932bf5-5fe8-4328-bd75-1761dfd58a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        for k, v in data.items():\n",
    "            text1 = str(v[\"claim_text\"]).strip()\n",
    "            for evi in v[\"evidences\"]:\n",
    "                text2 = str(evidence_dict[evi]).strip()\n",
    "                label = 1\n",
    "                texts.append([text1, text2])\n",
    "                labels.append(label)\n",
    "                ran = random.choice(all_evidences)\n",
    "                while ran in v[\"evidences\"]:\n",
    "                    ran = random.choice(all_evidences)\n",
    "                texts.append([text1, evidence_dict[ran]])\n",
    "                labels.append(0)\n",
    "    assert len(texts) == len(labels)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1293be60-72b6-4b5c-8f2c-c018c6bc5d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_texts, train_labels = read_data(\"project-data/train-claims.json\")\n",
    "val_texts, val_labels = read_data(\"project-data/dev-claims.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c454dde-1fdf-412e-a2b2-7c5b39aafd5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8244, 982)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452cd78c",
   "metadata": {},
   "source": [
    "### 5.查看text和label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "018f6ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.',\n",
       "  'At very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.'],\n",
       " ['Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.',\n",
       "  '\"It\\'s a fact: climate change made Hurricane Harvey more deadly\".'],\n",
       " ['Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.',\n",
       "  'Plants can grow as much as 50 percent faster in concentrations of 1,000 ppm CO 2 when compared with ambient conditions, though this assumes no change in climate and no limitation on other nutrients.']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdd3548a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425dd89d",
   "metadata": {},
   "source": [
    "### 6.求最大长度，为后面分词做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f198dc58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(item) for item in train_texts])\n",
    "print(max_len)\n",
    "\n",
    "max_len = max([len(item) for item in val_texts])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43887f5",
   "metadata": {},
   "source": [
    "### 7. label和id进行映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "808fa996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    item: idx for idx, item in enumerate(sorted(set(train_labels + val_labels)))\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc28d5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 0, 1: 1}, {0: 0, 1: 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id, id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b198e0d7",
   "metadata": {},
   "source": [
    "### 8.训练集和验证集 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e69e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(\n",
    "    [i[0] for i in train_texts],\n",
    "    [i[1] for i in train_texts],\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=256,\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    [i[0] for i in val_texts],\n",
    "    [i[1] for i in val_texts],\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfff268",
   "metadata": {},
   "source": [
    "### 9.创建Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "943ca92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CuDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8b6d40a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = CuDataset(train_encodings, train_labels)\n",
    "val_dataset = CuDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5425deca",
   "metadata": {},
   "source": [
    "### 10.创建Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd00e5db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec65736",
   "metadata": {},
   "source": [
    "### 11.加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e372a32a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=len(label2id)\n",
    ")\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")  # 使用cpu或者gpu\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb07d6",
   "metadata": {},
   "source": [
    "### 12.计算Accuracy，Precision，Recall，F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e19db358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(labels, preds):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average=\"macro\")\n",
    "    recall = recall_score(labels, preds, average=\"macro\")\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    logging.info(f\"accuracy: {accuracy}\")\n",
    "    logging.info(f\"precision: {precision}\")\n",
    "    logging.info(f\"recall: {recall}\")\n",
    "    logging.info(f\"f1: {f1}\\n\")\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41134c",
   "metadata": {},
   "source": [
    "### 13.评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c441ac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_model(model, eval_loader):\n",
    "    model.eval()\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for idx, batch in enumerate(eval_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels.extend(batch[\"labels\"].numpy())\n",
    "        outputs = model(\n",
    "            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
    "        )  # 输出所有概率\n",
    "        preds.extend(torch.argmax(outputs[0], dim=-1).cpu().numpy())  # 拿到标签\n",
    "    macro_f = compute_metrics(labels, preds)\n",
    "    model.train()\n",
    "    return macro_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09288c1",
   "metadata": {},
   "source": [
    "### 14.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d8ef1c4-20fd-4321-89c9-d513b352bade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b06c8d636f4631ae4d804bf1787320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7ac82d47ff4386b33a88699455828c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay_rate\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay_rate\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "\n",
    "step = 0\n",
    "best_acc = 0\n",
    "epoch = 2\n",
    "writer = SummaryWriter(log_dir=\"model_best\")\n",
    "for epoch in tqdm(range(epoch)):\n",
    "    for idx, batch in tqdm(\n",
    "        enumerate(train_loader), total=len(train_texts) // batch_size, leave=False\n",
    "    ):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs[0]  # 计算Loss\n",
    "        logging.info(f\"Epoch-{epoch}, Step-{step}, Loss: {loss.cpu().detach().numpy()}\")\n",
    "        step += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), step)\n",
    "    logging.info(f\"Epoch {epoch}, present best acc: {best_acc}, start evaluating.\")\n",
    "    accuracy, precision, recall, f1 = eval_model(model, eval_loader)  # 评估模型\n",
    "    writer.add_scalar(\"dev_accuracy\", accuracy, step)\n",
    "    writer.add_scalar(\"dev_precision\", precision, step)\n",
    "    writer.add_scalar(\"dev_recall\", recall, step)\n",
    "    writer.add_scalar(\"dev_f1\", f1, step)\n",
    "    if accuracy > best_acc:\n",
    "        model.save_pretrained(\"model_best\")  # 保存模型\n",
    "        tokenizer.save_pretrained(\"model_best\")\n",
    "        best_acc = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9dd753-1f9b-4252-a006-206da76e117b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ebcc9ab275a0b8333d76f3c9007aff6798cc23ea07d3b4c53c0b1df0392e66fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
