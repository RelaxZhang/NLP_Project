{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial.distance import cdist\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nlp_function import pick_random_keys, stopwords_func, lower_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in data\n",
    "# Read in training data (claim)\n",
    "with open('../project-data/train-claims.json', 'r') as tclaim_file:\n",
    "    tclaim_data = json.load(tclaim_file)\n",
    "\n",
    "# Read in development data (claim)\n",
    "with open('../project-data/dev-claims.json', 'r') as dclaim_file:\n",
    "    dclaim_data = json.load(dclaim_file)\n",
    "\n",
    "# Read in test data (claim)\n",
    "with open('../project-data/test-claims-unlabelled.json', 'r') as uclaim_file:\n",
    "    uclaim_data = json.load(uclaim_file)\n",
    "\n",
    "# Read in evidence data\n",
    "with open('../project-data/evidence.json', 'r') as evi_file:\n",
    "    evi_data = json.load(evi_file)\n",
    "\n",
    "## Preprocessing - Lowercase operation of the case\n",
    "tclaim_data = lower_processing(tclaim_data, \"claim_text\")\n",
    "dclaim_data = lower_processing(dclaim_data, \"claim_text\")\n",
    "uclaim_data = lower_processing(uclaim_data, \"claim_text\")\n",
    "evi_data = lower_processing(evi_data, 'evidence')\n",
    "\n",
    "# ## Remove stopwords from claims and evidence (optional)\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# tclaim_data = stopwords_func(stop_words, \"claim\", tclaim_data)\n",
    "# dclaim_data = stopwords_func(stop_words, \"claim\", dclaim_data)\n",
    "# uclaim_data = stopwords_func(stop_words, \"claim\", uclaim_data)\n",
    "# evi_data = stopwords_func(stop_words, \"evidence\", evi_data)\n",
    "\n",
    "## Create claim-evidence pair based on training set\n",
    "train_pairs = []\n",
    "for i in tclaim_data.values():\n",
    "    for j in i[\"evidences\"]:\n",
    "        train_pairs.append((i[\"claim_text\"], evi_data[j], 1))\n",
    "\n",
    "## insert negative sample to the training set\n",
    "for i in tclaim_data.values():\n",
    "    excluded_keys = i[\"evidences\"]\n",
    "    random_keys = pick_random_keys(evi_data, excluded_keys, len(excluded_keys))\n",
    "    for j in random_keys:\n",
    "        train_pairs.append((i[\"claim_text\"], evi_data[j], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain sentence list\n",
    "sentence_dict = {\"train\": 0, \"test\": 0}\n",
    "sentence_list = []\n",
    "for i in tclaim_data:\n",
    "    sentence_dict[\"train\"] += 1\n",
    "    sentence_list.append(tclaim_data[i][\"claim_text\"])\n",
    "for i in uclaim_data:\n",
    "    sentence_dict[\"test\"] += 1\n",
    "    sentence_list.append(uclaim_data[i][\"claim_text\"])\n",
    "\n",
    "# Load pre-trained SBERT model\n",
    "model_name = 'distilbert-base-nli-mean-tokens'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Embed sentences and obtain test set vectors\n",
    "embeddings = model.encode(sentence_list)\n",
    "train_matrix = embeddings[:sentence_dict[\"train\"]]\n",
    "test_matrix = embeddings[sentence_dict[\"train\"]:]\n",
    "\n",
    "# Capture the closest training instance (index) to the test set\n",
    "test_train_index = []\n",
    "for i in range(test_matrix.shape[0]):\n",
    "    distances = cdist(train_matrix, np.expand_dims(test_matrix[i], axis=0), metric='euclidean')\n",
    "    test_train_index.append(np.argmin(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-R Classification \n",
    "label_list = []\n",
    "for i in tclaim_data.values():\n",
    "    label_list.append(i[\"claim_label\"])\n",
    "strings = label_list\n",
    "counter = Counter(strings)\n",
    "most_common = counter.most_common(1)\n",
    "most_frequent_string = most_common[0][0]\n",
    "frequency = most_common[0][1]\n",
    "\n",
    "# Assign label and evidence to the test set\n",
    "train_key_list = list(tclaim_data.keys())\n",
    "count = 0\n",
    "for i in uclaim_data.values():\n",
    "    i[\"claim_label\"] = most_frequent_string\n",
    "    i[\"evidences\"] = tclaim_data[train_key_list[test_train_index[count]]][\"evidences\"]\n",
    "    count += 1\n",
    "\n",
    "# Save the test set result\n",
    "file_path = '../test-claims-predictions.json'\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(uclaim_data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Set up a simple classifier for text matching\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(2 * model.config.hidden_size, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 2)  # Assuming binary classification (matched vs not matched)\n",
    ").to(device)\n",
    "\n",
    "# Set up optimizer and loss function\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training data\n",
    "train_data = train_pairs\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for claim, evidence, label in train_data:\n",
    "        # Tokenize and encode claim and evidence\n",
    "        claim_tokens = tokenizer.tokenize(claim)\n",
    "        evidence_tokens = tokenizer.tokenize(evidence)\n",
    "        encoded_data = tokenizer.encode_plus(claim_tokens, evidence_tokens, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        input_ids = encoded_data['input_ids'].to(device)\n",
    "        attention_mask = encoded_data['attention_mask'].to(device)\n",
    "\n",
    "        # Generate BERT embeddings\n",
    "        # Generate BERT embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            claim_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "            evidence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Concatenate claim and evidence embeddings\n",
    "        combined_embedding = torch.cat((claim_embedding, evidence_embedding), dim=1)\n",
    "\n",
    "        # Make predictions\n",
    "        logits = classifier(combined_embedding)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(logits, torch.tensor([label]).to(device))\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Average Loss = {total_loss / len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "test_data = []\n",
    "for i in dclaim_data.values():\n",
    "    for j in i[\"evidences\"]:\n",
    "        test_data.append((i[\"claim_text\"], evi_data[j]))\n",
    "\n",
    "count = 0\n",
    "# Testing loop\n",
    "for claim, evidence in test_data:\n",
    "    # Tokenize and encode claim and evidence\n",
    "    claim_tokens = tokenizer.tokenize(claim)\n",
    "    evidence_tokens = tokenizer.tokenize(evidence)\n",
    "    encoded_data = tokenizer.encode_plus(claim_tokens, evidence_tokens, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    input_ids = encoded_data['input_ids'].to(device)\n",
    "    attention_mask = encoded_data['attention_mask'].to(device)\n",
    "\n",
    "    # Generate BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        claim_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        evidence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "    # Concatenate claim and evidence embeddings\n",
    "    combined_embedding = torch.cat((claim_embedding, evidence_embedding), dim=1)\n",
    "\n",
    "    # Make predictions\n",
    "    logits = classifier(combined_embedding)\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    if predictions.item() == 1:\n",
    "        count += 0\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Claim: {claim}\")\n",
    "    print(f\"Evidence: {evidence}\")\n",
    "    print(f\"Prediction: {predictions.item()}\")\n",
    "    print()\n",
    "\n",
    "count / len(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
