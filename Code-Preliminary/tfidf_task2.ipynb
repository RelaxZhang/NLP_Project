{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/relax/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/relax/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial.distance import cdist\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Remove stopwords from claim and evidence for reducing the computational consumption'''\n",
    "def stopwords_func(stop_words, text_type, text_data):\n",
    "    if text_type == \"evidence\":\n",
    "        for i in text_data:\n",
    "            sentence = text_data[i]\n",
    "            words = sentence.split()\n",
    "            filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "            filtered_sentence = \" \".join(filtered_words)\n",
    "            text_data[i] = filtered_sentence\n",
    "    else:\n",
    "        for i in text_data.values():\n",
    "            sentence = i[\"claim_text\"]\n",
    "            words = sentence.split()\n",
    "            filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "            filtered_sentence = \" \".join(filtered_words)\n",
    "            i[\"claim_text\"] = filtered_sentence\n",
    "    return text_data\n",
    "\n",
    "'''Function for picking random keys from the dictionary after excluding the specified key(s)'''\n",
    "def pick_random_keys(dictionary, excluded_keys, num_keys):\n",
    "    available_keys = [key for key in dictionary.keys() if key not in excluded_keys]\n",
    "    random_keys = random.sample(available_keys, num_keys)\n",
    "    return random_keys\n",
    "\n",
    "'''Function for turning the text into lowercase expression'''\n",
    "def lower_processing(data, text_type):\n",
    "    if text_type == \"claim_text\":\n",
    "        for i in data:\n",
    "            data[i][text_type] = data[i][text_type].lower()\n",
    "    else:\n",
    "        for i in data:\n",
    "            data[i] = data[i].lower()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in data\n",
    "# Read in training data (claim)\n",
    "with open('../project-data/train-claims.json', 'r') as tclaim_file:\n",
    "    tclaim_data = json.load(tclaim_file)\n",
    "\n",
    "# Read in development data (claim)\n",
    "with open('../project-data/dev-claims.json', 'r') as dclaim_file:\n",
    "    dclaim_data = json.load(dclaim_file)\n",
    "\n",
    "# Read in test data (claim)\n",
    "with open('../project-data/test-claims-unlabelled.json', 'r') as uclaim_file:\n",
    "    uclaim_data = json.load(uclaim_file)\n",
    "\n",
    "# Read in evidence data\n",
    "with open('../project-data/evidence.json', 'r') as evi_file:\n",
    "    evi_data = json.load(evi_file)\n",
    "\n",
    "## Preprocessing - Lowercase operation of the case\n",
    "tclaim_data = lower_processing(tclaim_data, \"claim_text\")\n",
    "dclaim_data = lower_processing(dclaim_data, \"claim_text\")\n",
    "uclaim_data = lower_processing(uclaim_data, \"claim_text\")\n",
    "evi_data = lower_processing(evi_data, 'evidence')\n",
    "\n",
    "## Create claim-evidence pair based on training set\n",
    "tkey_list = list(tclaim_data.keys())\n",
    "train_pairs = []\n",
    "evi_keys = []\n",
    "labels = []\n",
    "for i in tclaim_data:\n",
    "    for j in tclaim_data[i][\"evidences\"]:\n",
    "        evi_keys.append(j)\n",
    "        train_pairs.append((i, tclaim_data[i][\"claim_text\"], evi_data[j], j))\n",
    "        labels.append(1)\n",
    "  \n",
    "## insert negative sample to the training set\n",
    "random.seed(1)\n",
    "for i in tclaim_data:\n",
    "    excluded_keys = tclaim_data[i][\"evidences\"]\n",
    "    random_keys = pick_random_keys(evi_data, excluded_keys, len(excluded_keys))\n",
    "    for j in random_keys:\n",
    "        evi_keys.append(j)\n",
    "        train_pairs.append((i, tclaim_data[i][\"claim_text\"], evi_data[j], j))\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the claim, evidence sentence(s) to list for embedded usage\n",
    "claim_train_sentence = []\n",
    "claim_dep_sentence = []\n",
    "claim_test_sentence = []\n",
    "evidence_sample_sentence = []\n",
    "\n",
    "for i in tclaim_data.values():\n",
    "    claim_train_sentence.append(i[\"claim_text\"])\n",
    "for i in dclaim_data.values():\n",
    "    claim_dep_sentence.append(i[\"claim_text\"])\n",
    "for i in uclaim_data.values():\n",
    "    claim_test_sentence.append(i[\"claim_text\"])\n",
    "tfidf_keys = []\n",
    "for i in train_pairs[:int(len(train_pairs)/2)]:\n",
    "  if i[2] not in evidence_sample_sentence:\n",
    "    evidence_sample_sentence.append(i[2])\n",
    "    tfidf_keys.append(i[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample training data\n",
    "train_data = claim_train_sentence + claim_dep_sentence\n",
    "\n",
    "# Sample test data\n",
    "test_data = claim_dep_sentence\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "tfidf_vectorizer.fit(train_data)\n",
    "\n",
    "# Transform the training and test data using the trained vectorizer\n",
    "train_embeddings = tfidf_vectorizer.transform(train_data).toarray()\n",
    "test_embeddings = tfidf_vectorizer.transform(test_data).toarray()\n",
    "\n",
    "# Create training and test np.ndarray\n",
    "train_np = train_embeddings[:len(claim_train_sentence)]\n",
    "evi_np = train_embeddings[len(claim_train_sentence):]\n",
    "test_np = test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample training data\n",
    "train_data = claim_train_sentence + claim_dep_sentence\n",
    "\n",
    "# Sample test data\n",
    "test_data = claim_test_sentence\n",
    "\n",
    "# Initialize BOW vectorizer and fit on training text data\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_data)\n",
    "\n",
    "# Transform new sentences into BOW vectors\n",
    "train_embeddings = vectorizer.transform(train_data).toarray()\n",
    "test_embeddings = vectorizer.transform(test_data)\n",
    "\n",
    "# Create training and test np.ndarray\n",
    "train_np = train_embeddings[:len(claim_train_sentence)]\n",
    "test_np = test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect similarity value for weighted operation\n",
    "label_list = []\n",
    "sim_list = []\n",
    "for i in test_np:\n",
    "    similarity = []\n",
    "    for j in train_np:\n",
    "        similarity.append(cosine_similarity(np.reshape(i, (1, -1)), np.reshape(j, (1, -1)))[0][0])\n",
    "    top_index = np.argsort(similarity)[-9:]\n",
    "    sim_list.append([similarity[k] for k in top_index])\n",
    "    label_list.append(list(top_index))\n",
    "\n",
    "# Collect the potential label\n",
    "potential_label_list = []\n",
    "train_key_list = list(tclaim_data.keys())\n",
    "for i in range(len(label_list)):\n",
    "    label_list_potential = []\n",
    "    for j in label_list[i]:\n",
    "        label_list_potential.append(tclaim_data[train_key_list[j]][\"claim_label\"])\n",
    "    potential_label_list.append(label_list_potential)\n",
    "\n",
    "# Compute the weighted KNN result (dictionary) and select the label with the greatest value\n",
    "final_label = []\n",
    "for i in range(len(potential_label_list)):\n",
    "    score_dict = {}\n",
    "    for j in range(len(potential_label_list[i])):\n",
    "        if potential_label_list[i][j] not in score_dict:\n",
    "            score_dict[potential_label_list[i][j]] = sim_list[i][j] ** 2\n",
    "        else:\n",
    "            score_dict[potential_label_list[i][j]] += sim_list[i][j] ** 2\n",
    "    final_label.append(max(score_dict, key = score_dict.get))\n",
    "\n",
    "# Store the predicted label into the json file\n",
    "with open('../test-claims-predictions.json', 'r') as final_json:\n",
    "    final_test = json.load(final_json)\n",
    "test_key_list = list(final_test.keys())\n",
    "for i in range(len(test_key_list)):\n",
    "    final_test[test_key_list[i]][\"claim_label\"] = final_label[i]\n",
    "file_path = '../test-claims-predictions.json'\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(final_test, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect similarity value for weighted operation\n",
    "label_list = []\n",
    "for i in test_np:\n",
    "    similarity = []\n",
    "    for j in train_np:\n",
    "        similarity.append(cosine_similarity(np.reshape(i, (1, -1)), np.reshape(j, (1, -1)))[0][0])\n",
    "    top_index = np.argsort(similarity)[-11:]\n",
    "    label_list.append(list(top_index))\n",
    "\n",
    "with open('../project-data/dev-claims-test.json', 'r') as final_json:\n",
    "    final_test = json.load(final_json)\n",
    "\n",
    "def most_frequent_element(nums):\n",
    "    return max(set(nums), key=nums.count)\n",
    "\n",
    "potential_label_list = []\n",
    "train_key_list = list(tclaim_data.keys())\n",
    "test_key_list = list(final_test.keys())\n",
    "for i in range(len(label_list)):\n",
    "    label_list_potential = []\n",
    "    for j in label_list[i]:\n",
    "        label_list_potential.append(tclaim_data[train_key_list[j]][\"claim_label\"])\n",
    "    potential_label_list.append(label_list_potential)\n",
    "    test_class = most_frequent_element(label_list_potential)\n",
    "    # test_class = (tclaim_data[train_key_list[label_list[i]]][\"claim_label\"])\n",
    "    final_test[test_key_list[i]][\"claim_label\"] = test_class\n",
    "\n",
    "# Store to json\n",
    "file_path = '../project-data/dev-claims-test.json'\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(final_test, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../test-claims-predictions.json', 'r') as final_json:\n",
    "    test = json.load(final_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test == test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
