{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training data (claim)\n",
    "with open('../project-data/train-claims.json', 'r') as tclaim_file:\n",
    "    tclaim_data = json.load(tclaim_file)\n",
    "\n",
    "# Read in development data (claim)\n",
    "with open('../project-data/dev-claims.json', 'r') as dclaim_file:\n",
    "    dclaim_data = json.load(dclaim_file)\n",
    "\n",
    "# Read in test data (claim)\n",
    "with open('../project-data/test-claims-unlabelled.json', 'r') as uclaim_file:\n",
    "    uclaim_data = json.load(uclaim_file)\n",
    "\n",
    "# Read in evidence data\n",
    "with open('../project-data/evidence.json', 'r') as evi_file:\n",
    "    evi_data = json.load(evi_file)\n",
    "evi_keys = list(evi_data.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training & Development Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create positive claim-evidence pair based on training set\n",
    "random.seed(1)\n",
    "train_pairs = []\n",
    "train_labels = []\n",
    "for i in tclaim_data.values():\n",
    "    for j in i[\"evidences\"]:\n",
    "        train_pairs.append([i[\"claim_text\"], evi_data[j]])\n",
    "        train_labels.append(1)\n",
    "\n",
    "# Insert negative sample to the training set\n",
    "for i in tclaim_data.values():\n",
    "    available_keys = [num for num in evi_keys if num not in i[\"evidences\"]]\n",
    "    random_keys = random.sample(available_keys, len(i[\"evidences\"]))\n",
    "    for j in random_keys:\n",
    "        train_pairs.append([i[\"claim_text\"], evi_data[j]])\n",
    "        train_labels.append(0)\n",
    "\n",
    "# Create list of sentence (training and evidence)\n",
    "train_claim_sentence_list = []\n",
    "train_evi_sentence_list = []\n",
    "for i in train_pairs:\n",
    "    train_claim_sentence_list.append(i[0])\n",
    "    train_evi_sentence_list.append(i[1])\n",
    "\n",
    "# Create positive claim-evidence pair based on development set\n",
    "dev_pairs = []\n",
    "dev_labels = []\n",
    "for i in dclaim_data.values():\n",
    "    for j in i[\"evidences\"]:\n",
    "        dev_pairs.append([i[\"claim_text\"], evi_data[j]])\n",
    "        dev_labels.append(1)\n",
    "\n",
    "# Insert negative sample to the training set\n",
    "for i in dclaim_data.values():\n",
    "    available_keys = [num for num in evi_keys if num not in i[\"evidences\"]]\n",
    "    random_keys = random.sample(available_keys, len(i[\"evidences\"]))\n",
    "    for j in random_keys:\n",
    "        dev_pairs.append([i[\"claim_text\"], evi_data[j]])\n",
    "        dev_labels.append(0)\n",
    "\n",
    "# Create list of sentence (training, dev, test claim and evidence)\n",
    "dev_claim_sentence_list = []\n",
    "dev_evi_sentence_list = []\n",
    "for i in dev_pairs:\n",
    "    dev_claim_sentence_list.append(i[0])\n",
    "    dev_evi_sentence_list.append(i[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Pre-trained Bert for Fine-Tuning Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = len(set(train_labels)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dataset Structure & Obtain DataLoader for Batch Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, claims, evidences, labels):\n",
    "        self.claims = claims\n",
    "        self.evidences = evidences\n",
    "        encoding = tokenizer(claims, evidences, padding='max_length', truncation=True, max_length = 256)\n",
    "        self.encoding = encoding\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.claims)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instance = {}\n",
    "        for key_type, tokens in self.encoding.items():\n",
    "            instance[key_type] = torch.tensor(tokens[idx])\n",
    "        instance[\"label\"] = torch.tensor(self.labels[idx])\n",
    "        return instance\n",
    "\n",
    "# Create training and development datasets\n",
    "train_data = MyDataset(train_claim_sentence_list, train_evi_sentence_list, train_labels)\n",
    "dev_data = MyDataset(dev_claim_sentence_list, dev_evi_sentence_list, dev_labels)\n",
    "\n",
    "# Create DataLoader for training and development sets\n",
    "train_data_loader = DataLoader(train_data, batch_size = 16, shuffle = True)\n",
    "dev_data_loader = DataLoader(dev_data, batch_size = 16, shuffle = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training the Bert Model (Fine-Tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the GPU (cuda in Colab) as device and start training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 10\n",
    "performance = 0\n",
    "\n",
    "# train the model with 10 epoches\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Split the dataset into random batches for training without overflowing the RAM / GPU\n",
    "    for batch in enumerate(train_data_loader):\n",
    "        batch_info = batch[1]\n",
    "        input_ids = batch_info['input_ids'].to(device)\n",
    "        attention_mask = batch_info['attention_mask'].to(device)\n",
    "        token_type_ids = batch_info[\"token_type_ids\"].to(device)\n",
    "        labels = batch_info['label'].to(device)\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids, labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Apply zero_grad to remove the gradient from previous training\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Check the performance with development set\n",
    "    model.eval()\n",
    "    dev_labels = []\n",
    "    pred_labels = []\n",
    "    for dev_batch in enumerate(dev_data_loader):\n",
    "        # Load the development pairs (pos / neg)\n",
    "        dev_batch_info = dev_batch[1]\n",
    "        dev_input_ids = dev_batch_info['input_ids'].to(device)\n",
    "        dev_attention_mask = dev_batch_info['attention_mask'].to(device)\n",
    "        dev_token_type_ids = dev_batch_info[\"token_type_ids\"].to(device)\n",
    "        dev_labels += list(dev_batch[\"label\"].numpy())\n",
    "        \n",
    "        # Predict the development set evidence-claim pairs\n",
    "        outputs = model(dev_input_ids, dev_attention_mask, dev_token_type_ids)\n",
    "        pred_labels += list(torch.argmax(outputs[0], dim = -1).cpu().numpy())\n",
    "    \n",
    "    # Keep training the model and save the current optimal model\n",
    "    model.train()\n",
    "    current_performance = f1_score(dev_labels, pred_labels)\n",
    "    if current_performance >= performance:\n",
    "        performance = current_performance\n",
    "        tokenizer.save_pretrained(\"fine_tuned_bert_model\")\n",
    "        model.save_pretrained(\"fine_tuned_bert_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
