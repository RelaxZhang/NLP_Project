{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 该文档需要的包（这些都是能够调用的Model，也可以从我微信中提到的 Hugging Face Link / Spec 调取其他有效模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial.distance import cdist\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些用于数据预处理的Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Remove stopwords from claim and evidence for reducing the computational consumption'''\n",
    "def stopwords_func(stop_words, text_type, text_data):\n",
    "    if text_type == \"evidence\":\n",
    "        for i in text_data:\n",
    "            sentence = text_data[i]\n",
    "            words = sentence.split()\n",
    "            filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "            filtered_sentence = \" \".join(filtered_words)\n",
    "            text_data[i] = filtered_sentence\n",
    "    else:\n",
    "        for i in text_data.values():\n",
    "            sentence = i[\"claim_text\"]\n",
    "            words = sentence.split()\n",
    "            filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "            filtered_sentence = \" \".join(filtered_words)\n",
    "            i[\"claim_text\"] = filtered_sentence\n",
    "    return text_data\n",
    "\n",
    "'''Function for picking random keys from the dictionary after excluding the specified key(s)'''\n",
    "def pick_random_keys(dictionary, excluded_keys, num_keys):\n",
    "    available_keys = [key for key in dictionary.keys() if key not in excluded_keys]\n",
    "    random_keys = random.sample(available_keys, num_keys)\n",
    "    return random_keys\n",
    "\n",
    "'''Function for turning the text into lowercase expression'''\n",
    "def lower_processing(data, text_type):\n",
    "    if text_type == \"claim_text\":\n",
    "        for i in data:\n",
    "            data[i][text_type] = data[i][text_type].lower()\n",
    "    else:\n",
    "        for i in data:\n",
    "            data[i] = data[i].lower()\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 读数据（path需要调整），全体小写，扔掉Stopword（被Comment），将Training-set中的claim-evidence拆解成一一对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in data\n",
    "# Read in training data (claim)\n",
    "with open('../project-data/train-claims.json', 'r') as tclaim_file:\n",
    "    tclaim_data = json.load(tclaim_file)\n",
    "\n",
    "# Read in development data (claim)\n",
    "with open('../project-data/dev-claims.json', 'r') as dclaim_file:\n",
    "    dclaim_data = json.load(dclaim_file)\n",
    "\n",
    "# Read in test data (claim)\n",
    "with open('../project-data/test-claims-unlabelled.json', 'r') as uclaim_file:\n",
    "    uclaim_data = json.load(uclaim_file)\n",
    "\n",
    "# Read in evidence data\n",
    "with open('../project-data/evidence.json', 'r') as evi_file:\n",
    "    evi_data = json.load(evi_file)\n",
    "\n",
    "## Preprocessing - Lowercase operation of the case\n",
    "tclaim_data = lower_processing(tclaim_data, \"claim_text\")\n",
    "dclaim_data = lower_processing(dclaim_data, \"claim_text\")\n",
    "uclaim_data = lower_processing(uclaim_data, \"claim_text\")\n",
    "evi_data = lower_processing(evi_data, 'evidence')\n",
    "\n",
    "# ## Remove stopwords from claims and evidence (optional)\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# tclaim_data = stopwords_func(stop_words, \"claim\", tclaim_data)\n",
    "# dclaim_data = stopwords_func(stop_words, \"claim\", dclaim_data)\n",
    "# uclaim_data = stopwords_func(stop_words, \"claim\", uclaim_data)\n",
    "# evi_data = stopwords_func(stop_words, \"evidence\", evi_data)\n",
    "\n",
    "## Create claim-evidence pair based on training set\n",
    "train_pairs = []\n",
    "for i in tclaim_data.values():\n",
    "    for j in i[\"evidences\"]:\n",
    "        train_pairs.append((i[\"claim_text\"], evi_data[j], 1))\n",
    "\n",
    "## insert negative sample to the training set\n",
    "for i in tclaim_data.values():\n",
    "    excluded_keys = i[\"evidences\"]\n",
    "    random_keys = pick_random_keys(evi_data, excluded_keys, len(excluded_keys))\n",
    "    for j in random_keys:\n",
    "        train_pairs.append((i[\"claim_text\"], evi_data[j], 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 尝试用bert-latge-uncased pretrain model 来进行 sentence embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_sample_sentence = []\n",
    "for i in train_pairs:\n",
    "  evidence_sample_sentence.append(i[1])\n",
    "\n",
    "# # Apply the bert pre-trained model to embed the claim and evidence sentence\n",
    "device = torch.device(\"cuda\")\n",
    "model_name = \"bert-large-uncased\"\n",
    "model = BertModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "encoded_inputs = tokenizer.batch_encode_plus(evidence_sample_sentence, add_special_tokens=True, padding='longest', truncation=True, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_inputs)\n",
    "    sample_evidence_embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "print(sample_evidence_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
