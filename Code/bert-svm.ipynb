{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial.distance import cdist\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nlp_function import pick_random_keys, stopwords_func, lower_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in data\n",
    "# Read in training data (claim)\n",
    "with open('../project-data/train-claims.json', 'r') as tclaim_file:\n",
    "    tclaim_data = json.load(tclaim_file)\n",
    "\n",
    "# Read in development data (claim)\n",
    "with open('../project-data/dev-claims.json', 'r') as dclaim_file:\n",
    "    dclaim_data = json.load(dclaim_file)\n",
    "\n",
    "# Read in test data (claim)\n",
    "with open('../project-data/test-claims-unlabelled.json', 'r') as uclaim_file:\n",
    "    uclaim_data = json.load(uclaim_file)\n",
    "\n",
    "# Read in evidence data\n",
    "with open('../project-data/evidence.json', 'r') as evi_file:\n",
    "    evi_data = json.load(evi_file)\n",
    "\n",
    "## Preprocessing - Lowercase operation of the case\n",
    "tclaim_data = lower_processing(tclaim_data, \"claim_text\")\n",
    "dclaim_data = lower_processing(dclaim_data, \"claim_text\")\n",
    "uclaim_data = lower_processing(uclaim_data, \"claim_text\")\n",
    "evi_data = lower_processing(evi_data, 'evidence')\n",
    "\n",
    "# ## Remove stopwords from claims and evidence (optional)\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# tclaim_data = stopwords_func(stop_words, \"claim\", tclaim_data)\n",
    "# dclaim_data = stopwords_func(stop_words, \"claim\", dclaim_data)\n",
    "# uclaim_data = stopwords_func(stop_words, \"claim\", uclaim_data)\n",
    "# evi_data = stopwords_func(stop_words, \"evidence\", evi_data)\n",
    "\n",
    "## Create claim-evidence pair based on training set\n",
    "train_pairs = []\n",
    "for i in tclaim_data.values():\n",
    "    for j in i[\"evidences\"]:\n",
    "        train_pairs.append((i[\"claim_text\"], evi_data[j], 1))\n",
    "\n",
    "## insert negative sample to the training set\n",
    "for i in tclaim_data.values():\n",
    "    excluded_keys = i[\"evidences\"]\n",
    "    random_keys = pick_random_keys(evi_data, excluded_keys, len(excluded_keys))\n",
    "    for j in random_keys:\n",
    "        train_pairs.append((i[\"claim_text\"], evi_data[j], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/relax/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/relax/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_train_sentence = []\n",
    "claim_dep_sentence = []\n",
    "claim_test_sentence = []\n",
    "evidence_sentence = []\n",
    "\n",
    "for i in tclaim_data.values():\n",
    "    claim_train_sentence.append(i[\"claim_text\"])\n",
    "for i in dclaim_data.values():\n",
    "    claim_dep_sentence.append(i[\"claim_text\"])\n",
    "for i in uclaim_data.values():\n",
    "    claim_test_sentence.append(i[\"claim_text\"])\n",
    "for i in evi_data:\n",
    "    evidence_sentence.append(evi_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e08a5f679b847dca7cff1662b55b87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac08c13df45b46bc8f8bb5511f238ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31065f3b38b4b6f9c59fc9ac4aa49af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50256eb210c642cb80b057e93c92f8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1228, 1024])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-large-uncased\"\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "encoded_inputs = tokenizer.batch_encode_plus(claim_train_sentence, add_special_tokens=True, padding='longest', truncation=True, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_inputs)\n",
    "    sentence_embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.39607003, -0.37619188, -0.7186759 , ..., -0.8095297 ,\n",
       "        -0.6317222 ,  0.10742587],\n",
       "       [-0.23124436,  0.28230086, -0.56114906, ..., -0.9730543 ,\n",
       "        -0.5901787 ,  0.35957515],\n",
       "       [-0.08257185, -0.06848834, -0.19044401, ..., -0.20861049,\n",
       "        -0.58043325,  0.23053607],\n",
       "       ...,\n",
       "       [ 0.12806794,  0.30706802, -0.61280346, ..., -1.0072852 ,\n",
       "        -0.7599472 ,  0.20259118],\n",
       "       [-0.1665917 ,  0.2650316 , -0.94764674, ..., -0.8275705 ,\n",
       "        -0.5272113 , -0.48972073],\n",
       "       [-0.51467323, -0.12119047, -1.2438874 , ..., -0.06886862,\n",
       "        -1.4306076 ,  0.40594798]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_train_sentence = []\n",
    "claim_dep_sentence = []\n",
    "claim_test_sentence = []\n",
    "evidence_sentence = []\n",
    "\n",
    "for i in tclaim_data.values():\n",
    "    claim_train_sentence.append(i[\"claim_text\"])\n",
    "for i in dclaim_data.values():\n",
    "    claim_dep_sentence.append(i[\"claim_text\"])\n",
    "for i in uclaim_data.values():\n",
    "    claim_test_sentence.append(i[\"claim_text\"])\n",
    "for i in evi_data:\n",
    "    evidence_sentence.append(evi_data[i])\n",
    "\n",
    "# Load pre-trained SBERT model\n",
    "model_name = 'distilbert-base-nli-mean-tokens'\n",
    "model = SentenceTransformer(model_name, device=torch.device(\"mps\"))\n",
    "\n",
    "# Embed sentences and obtain test set vectors\n",
    "train_claim_embeddings = model.encode(claim_train_sentence)\n",
    "deveplop_claim_embeddings = model.encode(claim_dep_sentence)\n",
    "test_claim_embeddings = model.encode(claim_test_sentence)\n",
    "evidence_embeddings = model.encode(evidence_sentence)\n",
    "\n",
    "train_out_file = open(\"../train_claim.npz\", \"wb\")\n",
    "np.save(train_out_file, train_claim_embeddings)\n",
    "dev_out_file = open(\"../dev_claim.npz\", \"wb\")\n",
    "np.save(dev_out_file, deveplop_claim_embeddings)\n",
    "test_out_file = open(\"../test_claim.npz\", \"wb\")\n",
    "np.save(test_out_file, test_claim_embeddings)\n",
    "evi_out_file = open(\"../evidence.npz\", \"wb\")\n",
    "np.save(evi_out_file, evidence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain sentence list\n",
    "sentence_dict = {\"train\": 0, \"test\": 0}\n",
    "sentence_list = []\n",
    "for i in tclaim_data:\n",
    "    sentence_dict[\"train\"] += 1\n",
    "    sentence_list.append(tclaim_data[i][\"claim_text\"])\n",
    "for i in uclaim_data:\n",
    "    sentence_dict[\"test\"] += 1\n",
    "    sentence_list.append(uclaim_data[i][\"claim_text\"])\n",
    "\n",
    "# Load pre-trained SBERT model\n",
    "model_name = 'distilbert-base-nli-mean-tokens'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Embed sentences and obtain test set vectors\n",
    "embeddings = model.encode(sentence_list)\n",
    "train_matrix = embeddings[:sentence_dict[\"train\"]]\n",
    "test_matrix = embeddings[sentence_dict[\"train\"]:]\n",
    "\n",
    "# Capture the closest training instance (index) to the test set\n",
    "test_train_index = []\n",
    "for i in range(test_matrix.shape[0]):\n",
    "    distances = cdist(train_matrix, np.expand_dims(test_matrix[i], axis=0), metric='euclidean')\n",
    "    test_train_index.append(np.argmin(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-R Classification \n",
    "label_list = []\n",
    "for i in tclaim_data.values():\n",
    "    label_list.append(i[\"claim_label\"])\n",
    "strings = label_list\n",
    "counter = Counter(strings)\n",
    "most_common = counter.most_common(1)\n",
    "most_frequent_string = most_common[0][0]\n",
    "frequency = most_common[0][1]\n",
    "\n",
    "# Assign label and evidence to the test set\n",
    "train_key_list = list(tclaim_data.keys())\n",
    "count = 0\n",
    "for i in uclaim_data.values():\n",
    "    i[\"claim_label\"] = most_frequent_string\n",
    "    i[\"evidences\"] = tclaim_data[train_key_list[test_train_index[count]]][\"evidences\"]\n",
    "    count += 1\n",
    "\n",
    "# Save the test set result\n",
    "file_path = '../test-claims-predictions.json'\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(uclaim_data, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
