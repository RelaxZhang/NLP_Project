{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AdamW, BertForSequenceClassification, BertTokenizer\n",
    "from nlp_function import pick_random_keys, stopwords_func, lower_processing, most_frequent_element"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training data (claim)\n",
    "with open('../project-data/train-claims.json', 'r') as tclaim_file:\n",
    "    tclaim_data = json.load(tclaim_file)\n",
    "\n",
    "# Read in development data (claim)\n",
    "with open('../project-data/dev-claims.json', 'r') as dclaim_file:\n",
    "    dclaim_data = json.load(dclaim_file)\n",
    "\n",
    "# Read in test data (claim)\n",
    "with open('../project-data/test-claims-unlabelled.json', 'r') as uclaim_file:\n",
    "    uclaim_data = json.load(uclaim_file)\n",
    "\n",
    "# Read in evidence data\n",
    "with open('../project-data/evidence.json', 'r') as evi_file:\n",
    "    evi_data = json.load(evi_file)\n",
    "evi_keys = list(evi_data.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training & Development Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create positive claim-evidence pair based on training set\n",
    "random.seed(1)\n",
    "train_pairs = []\n",
    "train_labels = []\n",
    "evidence_keys = []\n",
    "for i in tclaim_data.values():\n",
    "    for j in i[\"evidences\"]:\n",
    "        evidence_keys.append(j)\n",
    "        train_pairs.append([i[\"claim_text\"], evi_data[j]])\n",
    "        train_labels.append(1)\n",
    "\n",
    "# Insert negative sample to the training set\n",
    "for i in tclaim_data.values():\n",
    "    available_keys = [num for num in evi_keys if num not in i[\"evidences\"]]\n",
    "    random_keys = random.sample(available_keys, len(i[\"evidences\"]))\n",
    "    for j in random_keys:\n",
    "        evidence_keys.append(j)\n",
    "        train_pairs.append([i[\"claim_text\"], evi_data[j]])\n",
    "        train_labels.append(0)\n",
    "\n",
    "# Create list of sentence (training and evidence)\n",
    "train_claim_sentence_list = []\n",
    "train_evi_sentence_list = []\n",
    "for i in train_pairs:\n",
    "    train_claim_sentence_list.append(i[0])\n",
    "    train_evi_sentence_list.append(i[1])\n",
    "\n",
    "# Create positive claim-evidence pair based on development set\n",
    "dev_pairs = []\n",
    "dev_labels = []\n",
    "for i in dclaim_data.values():\n",
    "    for j in i[\"evidences\"]:\n",
    "        evidence_keys.append(j)\n",
    "        dev_pairs.append([i[\"claim_text\"], evi_data[j]])\n",
    "        dev_labels.append(1)\n",
    "\n",
    "# Insert negative sample to the training set\n",
    "for i in dclaim_data.values():\n",
    "    available_keys = [num for num in evi_keys if num not in i[\"evidences\"]]\n",
    "    random_keys = random.sample(available_keys, len(i[\"evidences\"]))\n",
    "    for j in random_keys:\n",
    "        evidence_keys.append(j)\n",
    "        dev_pairs.append([i[\"claim_text\"], evi_data[j]])\n",
    "        dev_labels.append(0)\n",
    "\n",
    "# Create list of sentence (training, dev, test claim and evidence)\n",
    "dev_claim_sentence_list = []\n",
    "dev_evi_sentence_list = []\n",
    "for i in dev_pairs:\n",
    "    dev_claim_sentence_list.append(i[0])\n",
    "    dev_evi_sentence_list.append(i[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Pre-trained Bert for Fine-Tuning Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = len(set(train_labels)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dataset Structure & Obtain DataLoader for Batch Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "# Define Dataset\n",
    "# Implement based on the Tutorial of BERT (10-bert.ipynb)\n",
    "# https://canvas.lms.unimelb.edu.au/courses/151109/pages/worksheets-slash-notebooks?module_item_id=4589208\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, claims, evidences, labels):\n",
    "        self.claims = claims\n",
    "        self.evidences = evidences\n",
    "        encoding = tokenizer(claims, evidences, padding='max_length', truncation=True, max_length = 256)\n",
    "        self.encoding = encoding\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.claims)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instance = {}\n",
    "        for key_type, tokens in self.encoding.items():\n",
    "            instance[key_type] = torch.tensor(tokens[idx])\n",
    "        instance[\"label\"] = torch.tensor(self.labels[idx])\n",
    "        return instance\n",
    "\n",
    "# Create training and development datasets\n",
    "train_data = MyDataset(train_claim_sentence_list, train_evi_sentence_list, train_labels)\n",
    "dev_data = MyDataset(dev_claim_sentence_list, dev_evi_sentence_list, dev_labels)\n",
    "\n",
    "# Create DataLoader for training and development sets\n",
    "train_data_loader = DataLoader(train_data, batch_size = 16, shuffle = True)\n",
    "dev_data_loader = DataLoader(dev_data, batch_size = 16, shuffle = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training the Bert Model (Fine-Tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the GPU (cuda in Colab) as device and start training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 10\n",
    "performance = 0\n",
    "\n",
    "# train the model with 10 epoches\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Split the dataset into random batches for training without overflowing the RAM / GPU\n",
    "    for batch in enumerate(train_data_loader):\n",
    "        batch_info = batch[1]\n",
    "        input_ids = batch_info['input_ids'].to(device)\n",
    "        attention_mask = batch_info['attention_mask'].to(device)\n",
    "        token_type_ids = batch_info[\"token_type_ids\"].to(device)\n",
    "        labels = batch_info['label'].to(device)\n",
    "        outputs = model(input_ids = input_ids, attention_mask = attention_mask,\n",
    "                        token_type_ids = token_type_ids, labels = labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Apply zero_grad to remove the gradient from previous training\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Check the performance with development set\n",
    "    model.eval()\n",
    "    dev_labels = []\n",
    "    pred_labels = []\n",
    "    for dev_batch in enumerate(dev_data_loader):\n",
    "        # Load the development pairs (pos / neg)\n",
    "        dev_batch_info = dev_batch[1]\n",
    "        dev_input_ids = dev_batch_info['input_ids'].to(device)\n",
    "        dev_attention_mask = dev_batch_info['attention_mask'].to(device)\n",
    "        dev_token_type_ids = dev_batch_info[\"token_type_ids\"].to(device)\n",
    "        dev_labels.extend(dev_batch_info[\"label\"].numpy())\n",
    "        \n",
    "        # Predict the development set evidence-claim pairs\n",
    "        outputs = model(dev_input_ids, attention_mask = dev_attention_mask, token_type_ids = dev_token_type_ids)\n",
    "        pred_labels.extend(torch.argmax(outputs[0], dim = -1).cpu().numpy())\n",
    "    \n",
    "    # Keep training the model and save the current optimal model\n",
    "    model.train()\n",
    "    current_performance = f1_score(dev_labels, pred_labels)\n",
    "    if current_performance >= performance:\n",
    "        performance = current_performance\n",
    "        tokenizer.save_pretrained(\"../fine_tuned_bert_model\")\n",
    "        model.save_pretrained(\"../fine_tuned_bert_model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Evidence for Development Set Claim Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in Fine-Tuned Model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../fine_tuned_bert_model\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"../fine_tuned_bert_model\")\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain potential evidence sentence list\n",
    "potential_evidence = []\n",
    "for i in evidence_keys:\n",
    "    potential_evidence.append(evi_data[i])\n",
    "evidence_keys = list(set(evidence_keys))\n",
    "potential_evidence = list(set(potential_evidence))\n",
    "\n",
    "# Prepare test claim\n",
    "test_keys = list(uclaim_data.keys())\n",
    "test_sentence = []\n",
    "for i in uclaim_data.values():\n",
    "    test_sentence.append(i[\"claim_text\"])\n",
    "\n",
    "# Compute the similarity between the embedded claim and evidence\n",
    "k = 3\n",
    "for i in range(len(test_sentence)):\n",
    "    softmax_prob = []\n",
    "    for j in range(len(potential_evidence)):\n",
    "        pair_encode = tokenizer(test_sentence[i], potential_evidence[j], truncation = True, padding = 'max_length', max_length = 256)\n",
    "        test_ids = pair_encode[\"input_ids\"].to(device)\n",
    "        test_attention_mask = pair_encode[\"attention_mask\"].to(device)\n",
    "        test_token_type_ids = pair_encode[\"token_type_ids\"].to(device)\n",
    "        outputs = model(test_ids, test_attention_mask, test_token_type_ids)\n",
    "        probability = torch.softmax(model(test_ids, test_attention_mask, test_token_type_ids)[0], dim = -1)\n",
    "        softmax_prob.append(probability[:, 1].cpu().numpy()[0])\n",
    "    \n",
    "    sort_idx = np.argsort(softmax_prob)[::-1]\n",
    "    evidence_list = []\n",
    "    for evi in sort_idx[:k]:\n",
    "        evidence_list.append(potential_evidence[evi])\n",
    "    \n",
    "    # Assign the top-3 evidences to the claim text\n",
    "    uclaim_data[test_keys[i]]['evidences'] = evidence_list\n",
    "\n",
    "# Save the text file to json\n",
    "file_path = '../Performance/FineTune-Bert/test-claims-predictions.json'\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(uclaim_data, json_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the claim, evidence sentence(s) to list for embedded usage\n",
    "claim_train_sentence = []\n",
    "claim_dep_sentence = []\n",
    "claim_test_sentence = []\n",
    "evidence_full_sentence = []\n",
    "evidence_sample_sentence = []\n",
    "\n",
    "for i in tclaim_data.values():\n",
    "    claim_train_sentence.append(i[\"claim_text\"])\n",
    "for i in dclaim_data.values():\n",
    "    claim_dep_sentence.append(i[\"claim_text\"])\n",
    "for i in uclaim_data.values():\n",
    "    claim_test_sentence.append(i[\"claim_text\"])\n",
    "for i in evi_data.values():\n",
    "    evidence_full_sentence.append(i)\n",
    "tfidf_keys = []\n",
    "for i in train_pairs[:int(len(train_pairs)/2)]:\n",
    "  evidence_sample_sentence.append(i[2])\n",
    "  tfidf_keys.append(i[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "train_data = claim_train_sentence + claim_dep_sentence\n",
    "\n",
    "# Test data\n",
    "test_data = claim_test_sentence\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "tfidf_vectorizer.fit(train_data)\n",
    "\n",
    "# Transform the training and test data using the trained vectorizer\n",
    "train_embeddings = tfidf_vectorizer.transform(train_data).toarray()\n",
    "test_embeddings = tfidf_vectorizer.transform(test_data).toarray()\n",
    "\n",
    "# Create training and test np.ndarray\n",
    "train_np = train_embeddings[:len(claim_train_sentence)]\n",
    "test_np = test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect similarity value for mojority voting operation\n",
    "label_list = []\n",
    "for i in test_np:\n",
    "    similarity = []\n",
    "    for j in train_np:\n",
    "        similarity.append(cosine_similarity(np.reshape(i, (1, -1)), np.reshape(j, (1, -1)))[0][0])\n",
    "    top_index = np.argsort(similarity)[-11:]\n",
    "    label_list.append(list(top_index))\n",
    "\n",
    "with open('../Performance/FineTune-Bert/test-claims-predictions.json', 'r') as final_json:\n",
    "    final_test = json.load(final_json)\n",
    "\n",
    "# Obtain the most frequent label from the closest claim(s)\n",
    "potential_label_list = []\n",
    "train_key_list = list(tclaim_data.keys())\n",
    "test_key_list = list(final_test.keys())\n",
    "for i in range(len(label_list)):\n",
    "    label_list_potential = []\n",
    "    for j in label_list[i]:\n",
    "        label_list_potential.append(tclaim_data[train_key_list[j]][\"claim_label\"])\n",
    "    potential_label_list.append(label_list_potential)\n",
    "    test_class = most_frequent_element(label_list_potential)\n",
    "    final_test[test_key_list[i]][\"claim_label\"] = test_class\n",
    "\n",
    "# Store to json\n",
    "file_path = '../Performance/FineTune-Bert/test-claims-predictions.json'\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(final_test, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
